# Story 4.4: Chatbot Interaction with Knowledge

**Epic:** Epic 4 - Custom AI Chatbots
**Status:** Not Started
**Estimated Effort:** 12-16 hours
**Priority:** High
**Dependencies:** Stories 4.1, 4.2, 4.3

---

## User Story

**As a user**, I want to have conversations with my chatbot that are grounded in my uploaded documents, so that I receive accurate, contextual responses based on my specific knowledge base.

---

## Story Goal

Implement the complete RAG (Retrieval-Augmented Generation) pipeline that performs vector similarity search on uploaded documents, retrieves relevant context, and generates responses using Scaleway's inference API. Enable streaming responses and persistent chat history.

---

## Acceptance Criteria

### Chat Functionality
- [ ] User can send messages to active chatbot
- [ ] System retrieves top 5 relevant document chunks via vector search
- [ ] AI response generated using Scaleway Inference API
- [ ] Responses stream to UI in real-time
- [ ] Response time < 3 seconds for typical queries

### RAG Implementation
- [ ] Query embedding generated
- [ ] Vector similarity search using pgvector cosine similarity
- [ ] Top K chunks retrieved (K=5)
- [ ] Context window includes last 5 chat messages
- [ ] System prompt and context injected into LLM prompt

### Chat History
- [ ] All messages persisted to database
- [ ] Chat sessions created per chatbot
- [ ] Previous messages loaded when reopening chat
- [ ] Chat history scrollable and searchable

### Source Attribution
- [ ] User can see which documents informed response
- [ ] Source chunks displayed with relevance scores
- [ ] Clicking source opens document preview

### Error Handling
- [ ] Graceful handling of API failures
- [ ] User-friendly error messages
- [ ] Retry mechanism for transient failures
- [ ] Fallback when no relevant context found

---

## Dev Notes

### RAG Architecture

```typescript
// lib/rag.ts
import { prisma } from './prisma';
import { generateEmbedding } from './embeddings';
import { callScalewayInference } from './scaleway';

interface RAGContext {
  chunks: Array<{
    content: string;
    documentName: string;
    similarity: number;
  }>;
  systemPrompt: string;
  chatHistory: Array<{ role: string; content: string }>;
}

export async function performRAG(
  chatbotId: string,
  userMessage: string
): Promise<{ response: string; sources: any[] }> {
  // 1. Get chatbot configuration
  const chatbot = await prisma.chatbot.findUnique({
    where: { id: chatbotId },
    include: { knowledgeBase: true },
  });

  if (!chatbot) {
    throw new Error('Chatbot not found');
  }

  // 2. Generate query embedding
  const queryEmbedding = await generateEmbedding(userMessage);

  // 3. Vector similarity search
  const relevantChunks = await searchKnowledgeBase(
    chatbotId,
    queryEmbedding,
    5
  );

  // 4. Retrieve recent chat history
  const chatHistory = await getChatHistory(chatbotId, 5);

  // 5. Construct prompt
  const prompt = constructPrompt({
    chunks: relevantChunks,
    systemPrompt: chatbot.systemPrompt,
    chatHistory,
    userMessage,
  });

  // 6. Generate response
  const response = await callScalewayInference(prompt);

  return {
    response,
    sources: relevantChunks,
  };
}

async function searchKnowledgeBase(
  chatbotId: string,
  queryEmbedding: number[],
  topK: number
) {
  // Use pgvector for similarity search
  const result = await prisma.$queryRaw`
    SELECT
      dc.content,
      d.filename,
      1 - (dc.embedding <=> ${queryEmbedding}::vector) as similarity
    FROM "DocumentChunk" dc
    JOIN "Document" d ON d.id = dc."documentId"
    WHERE d."chatbotId" = ${chatbotId}
    ORDER BY dc.embedding <=> ${queryEmbedding}::vector
    LIMIT ${topK}
  `;

  return result;
}

function constructPrompt(context: RAGContext): string {
  let prompt = `System: ${context.systemPrompt}\n\n`;

  if (context.chunks.length > 0) {
    prompt += 'Context from your knowledge base:\n\n';
    context.chunks.forEach((chunk, idx) => {
      prompt += `[Document ${idx + 1}: ${chunk.documentName}]\n${chunk.content}\n\n`;
    });
  }

  if (context.chatHistory.length > 0) {
    prompt += 'Previous conversation:\n';
    context.chatHistory.forEach((msg) => {
      prompt += `${msg.role}: ${msg.content}\n`;
    });
  }

  return prompt;
}
```

### Scaleway Inference Integration

```typescript
// lib/scaleway.ts
const SCALEWAY_INFERENCE_ENDPOINT = 'https://api.scaleway.ai/v1/chat/completions';
const CHAT_MODEL = 'llama-3.1-70b-instruct';

export async function callScalewayInference(
  prompt: string,
  stream: boolean = false
): Promise<string | ReadableStream> {
  const response = await fetch(SCALEWAY_INFERENCE_ENDPOINT, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${process.env.SCALEWAY_API_KEY}`,
    },
    body: JSON.stringify({
      model: CHAT_MODEL,
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.7,
      max_tokens: 1000,
      stream,
    }),
  });

  if (!response.ok) {
    throw new Error(`Scaleway API error: ${response.statusText}`);
  }

  if (stream) {
    return response.body as ReadableStream;
  }

  const data = await response.json();
  return data.choices[0].message.content;
}
```

### Chat API with Streaming

```typescript
// app/api/chat/[chatbotId]/route.ts
import { NextRequest } from 'next/server';
import { getServerSession } from 'next-auth';
import { performRAG } from '@/lib/rag';

export async function POST(
  request: NextRequest,
  { params }: { params: { chatbotId: string } }
) {
  try {
    const session = await getServerSession();
    if (!session?.user?.id) {
      return new Response('Unauthorized', { status: 401 });
    }

    const { message } = await request.json();
    const { chatbotId } = params;

    // Verify chatbot ownership
    const chatbot = await prisma.chatbot.findFirst({
      where: { id: chatbotId, userId: session.user.id },
    });

    if (!chatbot) {
      return new Response('Chatbot not found', { status: 404 });
    }

    // Create chat session if needed
    let session = await getOrCreateChatSession(chatbotId);

    // Save user message
    await prisma.chatMessage.create({
      data: {
        sessionId: session.id,
        role: 'user',
        content: message,
      },
    });

    // Perform RAG
    const { response, sources } = await performRAG(chatbotId, message);

    // Save assistant response
    await prisma.chatMessage.create({
      data: {
        sessionId: session.id,
        role: 'assistant',
        content: response,
      },
    });

    return Response.json({
      data: {
        message: response,
        sources
      }
    });
  } catch (error) {
    console.error('Error in chat:', error);
    return new Response('Chat error', { status: 500 });
  }
}
```

### Frontend Integration

```typescript
// components/chat/ChatWindow.tsx (updated)
import { useChatStore } from '@/store/chatStore';

export function ChatWindow() {
  const { activeChatbotId, messages, addMessage, isLoading } = useChatStore();

  const handleSendMessage = async (content: string) => {
    // Add user message immediately
    addMessage(content, 'user');

    try {
      const response = await fetch(`/api/chat/${activeChatbotId}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: content }),
      });

      const data = await response.json();

      // Add assistant response
      addMessage(data.data.message, 'assistant', data.data.sources);
    } catch (error) {
      console.error('Chat error:', error);
      addMessage('Sorry, I encountered an error. Please try again.', 'assistant');
    }
  };

  // ... rest of component
}
```

---

## Tasks

1. **RAG Library** (4 hours)
   - [ ] Create `lib/rag.ts`
   - [ ] Implement vector search function
   - [ ] Implement prompt construction
   - [ ] Add context retrieval

2. **Scaleway Integration** (2 hours)
   - [ ] Create `lib/scaleway.ts`
   - [ ] Implement chat completions API call
   - [ ] Add streaming support
   - [ ] Error handling

3. **Chat API** (3 hours)
   - [ ] Create `app/api/chat/[chatbotId]/route.ts`
   - [ ] Implement POST endpoint
   - [ ] Integrate RAG pipeline
   - [ ] Save messages to database

4. **Chat History** (2 hours)
   - [ ] Add ChatSession and ChatMessage models
   - [ ] Implement history retrieval
   - [ ] Create `app/api/chat/[chatbotId]/history/route.ts`

5. **Frontend Updates** (3 hours)
   - [ ] Update ChatWindow with real API
   - [ ] Remove placeholder logic
   - [ ] Add loading states
   - [ ] Handle errors gracefully

6. **Source Attribution UI** (2 hours)
   - [ ] Create SourceAttribution component
   - [ ] Display source documents
   - [ ] Show relevance scores
   - [ ] Add document preview

7. **Testing** (2 hours)
   - [ ] Test RAG with various queries
   - [ ] Test with different document types
   - [ ] Test error scenarios
   - [ ] Performance testing

---

## Database Schema Updates

```prisma
model ChatSession {
  id              String    @id @default(cuid())
  chatbotId       String
  chatbot         Chatbot   @relation(fields: [chatbotId], references: [id], onDelete: Cascade)
  messages        ChatMessage[]
  createdAt       DateTime  @default(now())
  updatedAt       DateTime  @updatedAt

  @@index([chatbotId])
}

model ChatMessage {
  id              String      @id @default(cuid())
  sessionId       String
  session         ChatSession @relation(fields: [sessionId], references: [id], onDelete: Cascade)
  role            String      // 'user' or 'assistant'
  content         String      @db.Text
  sources         Json?       // Store source attribution
  timestamp       DateTime    @default(now())

  @@index([sessionId])
}
```

---

## Environment Variables
```bash
SCALEWAY_API_KEY=xxx
SCALEWAY_INFERENCE_ENDPOINT=https://api.scaleway.ai/v1
SCALEWAY_CHAT_MODEL=llama-3.1-70b-instruct
VECTOR_SEARCH_TOP_K=5
CHAT_HISTORY_CONTEXT_SIZE=5
MAX_RESPONSE_TOKENS=1000
```

---

## Definition of Done

- [ ] All acceptance criteria met
- [ ] RAG pipeline functional
- [ ] Vector search working correctly
- [ ] Scaleway API integrated
- [ ] Chat history persisted
- [ ] Source attribution displayed
- [ ] Streaming responses working
- [ ] Error handling implemented
- [ ] Performance targets met (<3s response)
- [ ] Code follows CLAUDE.md standards

---

**Epic 3 Complete!** 🎉
**Next Epic:** Epic 4 - School-Wide Knowledge Library
