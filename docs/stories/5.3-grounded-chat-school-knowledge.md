# Story 5.3: Grounded Chat with School Knowledge

**Epic:** Epic 5 - School-Wide Knowledge Library
**Status:** Not Started
**Estimated Effort:** 12-14 hours
**Priority:** High
**Dependencies:** Stories 5.1, 5.2, Epic 4 Complete

---

## User Story

**As a teacher**, I want my chatbot to draw from both my personal documents and linked school library materials, so that it provides responses grounded in both my supplementary resources and official institutional knowledge.

---

## Story Goal

Implement hybrid RAG that searches across both personal knowledge bases and linked school documents, intelligently merges results, and provides clear source attribution distinguishing personal from institutional sources.

---

## Acceptance Criteria

### Hybrid Search
- [ ] Chatbot searches both personal and school documents when both exist
- [ ] If only personal or only school docs exist, uses that source exclusively
- [ ] Search results weighted: 60% personal, 40% school (default)
- [ ] Top 5 chunks total retrieved (3 personal, 2 school by default)
- [ ] Weighting configurable per chatbot

### Response Generation
- [ ] AI prompt includes chunks from both sources
- [ ] Context clearly identifies source of each chunk
- [ ] Responses reference both personal and school materials when relevant
- [ ] Chat response time remains < 3 seconds

### Source Attribution
- [ ] Each message shows source badges (Personal/School)
- [ ] User can expand to see specific documents used
- [ ] Source list shows document names and relevance scores
- [ ] Clicking source opens document preview (school docs read-only)

### Updates Propagation
- [ ] If school admin updates document, changes immediately available
- [ ] Re-embedding happens automatically
- [ ] Old chunks replaced with new chunks
- [ ] Active chats continue seamlessly

### Performance
- [ ] Parallel vector searches (personal + school)
- [ ] Combined search completes in < 300ms
- [ ] No degradation in response quality
- [ ] Graceful fallback if one source unavailable

---

## Dev Notes

### Hybrid RAG Implementation

```typescript
// lib/rag-hybrid.ts
import { prisma } from './prisma';
import { generateEmbedding } from './embeddings';

interface RAGChunk {
  content: string;
  source: 'personal' | 'school';
  documentName: string;
  similarity: number;
}

export async function performHybridRAG(
  chatbotId: string,
  userMessage: string
): Promise<{ chunks: RAGChunk[]; weights: { personal: number; school: number } }> {
  // Get chatbot with linked docs
  const chatbot = await prisma.chatbot.findUnique({
    where: { id: chatbotId },
    include: {
      knowledgeBase: true,
      linkedSchoolDocs: {
        include: { document: true },
      },
    },
  });

  if (!chatbot) {
    throw new Error('Chatbot not found');
  }

  // Generate query embedding
  const queryEmbedding = await generateEmbedding(userMessage);

  // Parallel searches
  const [personalChunks, schoolChunks] = await Promise.all([
    searchPersonalKnowledge(chatbotId, queryEmbedding, 5),
    searchSchoolKnowledge(chatbotId, queryEmbedding, 5),
  ]);

  // Get weighting config (default 60/40)
  const weights = {
    personal: chatbot.personalWeight ?? 0.6,
    school: chatbot.schoolWeight ?? 0.4,
  };

  // Merge and weight results
  const merged = mergeResults(personalChunks, schoolChunks, weights);

  // Return top 5
  return {
    chunks: merged.slice(0, 5),
    weights,
  };
}

async function searchPersonalKnowledge(
  chatbotId: string,
  queryEmbedding: number[],
  topK: number
): Promise<RAGChunk[]> {
  const result = await prisma.$queryRaw<any[]>`
    SELECT
      dc.content,
      d.filename as "documentName",
      1 - (dc.embedding <=> ${queryEmbedding}::vector) as similarity,
      'personal' as source
    FROM "DocumentChunk" dc
    JOIN "Document" d ON d.id = dc."documentId"
    WHERE d."chatbotId" = ${chatbotId}
    ORDER BY dc.embedding <=> ${queryEmbedding}::vector
    LIMIT ${topK}
  `;

  return result.map((r) => ({
    content: r.content,
    source: 'personal' as const,
    documentName: r.documentName,
    similarity: r.similarity,
  }));
}

async function searchSchoolKnowledge(
  chatbotId: string,
  queryEmbedding: number[],
  topK: number
): Promise<RAGChunk[]> {
  const result = await prisma.$queryRaw<any[]>`
    SELECT
      sdc.content,
      sd.filename as "documentName",
      1 - (sdc.embedding <=> ${queryEmbedding}::vector) as similarity,
      'school' as source
    FROM "SchoolDocumentChunk" sdc
    JOIN "SchoolDocument" sd ON sd.id = sdc."documentId"
    JOIN "ChatbotSchoolDocLink" csdl ON csdl."documentId" = sd.id
    WHERE csdl."chatbotId" = ${chatbotId}
    ORDER BY sdc.embedding <=> ${queryEmbedding}::vector
    LIMIT ${topK}
  `;

  return result.map((r) => ({
    content: r.content,
    source: 'school' as const,
    documentName: r.documentName,
    similarity: r.similarity,
  }));
}

function mergeResults(
  personal: RAGChunk[],
  school: RAGChunk[],
  weights: { personal: number; school: number }
): RAGChunk[] {
  // Apply weights to similarity scores
  personal.forEach((chunk) => {
    chunk.similarity *= weights.personal;
  });

  school.forEach((chunk) => {
    chunk.similarity *= weights.school;
  });

  // Combine and sort by weighted similarity
  return [...personal, ...school].sort((a, b) => b.similarity - a.similarity);
}
```

### Updated Chat API

```typescript
// app/api/chat/[chatbotId]/route.ts (updated)
import { NextRequest, NextResponse } from 'next/server';
import { getServerSession } from 'next-auth';
import { performHybridRAG } from '@/lib/rag-hybrid';
import { callScalewayInference } from '@/lib/scaleway';
import { prisma } from '@/lib/prisma';

export async function POST(
  request: NextRequest,
  { params }: { params: { chatbotId: string } }
) {
  try {
    const session = await getServerSession();
    if (!session?.user?.id) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }

    const { message } = await request.json();
    const { chatbotId } = params;

    // Verify ownership
    const chatbot = await prisma.chatbot.findFirst({
      where: { id: chatbotId, userId: session.user.id },
    });

    if (!chatbot) {
      return NextResponse.json({ error: 'Chatbot not found' }, { status: 404 });
    }

    // Get or create chat session
    let chatSession = await prisma.chatSession.findFirst({
      where: { chatbotId },
      orderBy: { createdAt: 'desc' },
    });

    if (!chatSession) {
      chatSession = await prisma.chatSession.create({
        data: { chatbotId },
      });
    }

    // Save user message
    await prisma.chatMessage.create({
      data: {
        sessionId: chatSession.id,
        role: 'user',
        content: message,
      },
    });

    // Perform hybrid RAG
    const { chunks, weights } = await performHybridRAG(chatbotId, message);

    // Get chat history
    const history = await prisma.chatMessage.findMany({
      where: { sessionId: chatSession.id },
      orderBy: { timestamp: 'desc' },
      take: 5,
    });

    // Construct prompt
    const prompt = constructPrompt({
      systemPrompt: chatbot.systemPrompt,
      chunks,
      history: history.reverse(),
      userMessage: message,
    });

    // Generate response
    const response = await callScalewayInference(prompt);

    // Save assistant message with sources
    await prisma.chatMessage.create({
      data: {
        sessionId: chatSession.id,
        role: 'assistant',
        content: response,
        sources: chunks, // Store source attribution
      },
    });

    return NextResponse.json({
      data: {
        message: response,
        sources: chunks,
        weights,
      },
    });
  } catch (error) {
    console.error('Error in hybrid chat:', error);
    return NextResponse.json({ error: 'Chat error' }, { status: 500 });
  }
}

function constructPrompt(context: {
  systemPrompt: string;
  chunks: RAGChunk[];
  history: any[];
  userMessage: string;
}): string {
  let prompt = `System: ${context.systemPrompt}\n\n`;

  // Add context from both sources
  const personalChunks = context.chunks.filter((c) => c.source === 'personal');
  const schoolChunks = context.chunks.filter((c) => c.source === 'school');

  if (personalChunks.length > 0) {
    prompt += 'Context from your personal knowledge base:\n\n';
    personalChunks.forEach((chunk, idx) => {
      prompt += `[Personal Document ${idx + 1}: ${chunk.documentName}]\n${chunk.content}\n\n`;
    });
  }

  if (schoolChunks.length > 0) {
    prompt += 'Context from your school library:\n\n';
    schoolChunks.forEach((chunk, idx) => {
      prompt += `[School Document ${idx + 1}: ${chunk.documentName}]\n${chunk.content}\n\n`;
    });
  }

  // Add history
  if (context.history.length > 0) {
    prompt += 'Previous conversation:\n';
    context.history.forEach((msg) => {
      prompt += `${msg.role}: ${msg.content}\n`;
    });
  }

  prompt += `\nUser: ${context.userMessage}`;

  return prompt;
}
```

### Source Attribution UI

```typescript
// components/chat/SourceAttribution.tsx
import { Badge } from '@/components/ui/badge';
import { FileText, School } from 'lucide-react';

interface Source {
  content: string;
  source: 'personal' | 'school';
  documentName: string;
  similarity: number;
}

export function SourceAttribution({ sources }: { sources: Source[] }) {
  const [expanded, setExpanded] = useState(false);

  return (
    <div className="text-[var(--muted)] text-sm mt-2">
      <button
        onClick={() => setExpanded(!expanded)}
        className="flex items-center gap-2 hover:underline"
      >
        <FileText size={14} />
        {sources.length} sources
      </button>

      {expanded && (
        <div className="mt-2 space-y-2">
          {sources.map((source, idx) => (
            <div
              key={idx}
              className="flex items-start gap-2 p-2 bg-[var(--background)] border border-[var(--border)] rounded"
            >
              {source.source === 'school' ? (
                <School size={16} className="text-[var(--accent)] mt-0.5" />
              ) : (
                <FileText size={16} className="text-[var(--muted)] mt-0.5" />
              )}
              <div className="flex-1">
                <div className="flex items-center gap-2">
                  <span className="font-medium">{source.documentName}</span>
                  <Badge variant={source.source === 'school' ? 'default' : 'secondary'}>
                    {source.source === 'school' ? 'School' : 'Personal'}
                  </Badge>
                </div>
                <div className="text-xs text-[var(--muted)]">
                  Relevance: {(source.similarity * 100).toFixed(1)}%
                </div>
              </div>
            </div>
          ))}
        </div>
      )}
    </div>
  );
}
```

---

## Tasks

1. **Hybrid RAG Library** (4 hours)
   - [ ] Create `lib/rag-hybrid.ts`
   - [ ] Implement parallel search logic
   - [ ] Implement result merging and weighting
   - [ ] Add configuration options

2. **Update Chat API** (3 hours)
   - [ ] Update `/api/chat/[chatbotId]/route.ts`
   - [ ] Integrate hybrid RAG
   - [ ] Store source attribution
   - [ ] Update prompt construction

3. **Source Attribution UI** (2 hours)
   - [ ] Create SourceAttribution component
   - [ ] Add source badges to messages
   - [ ] Implement expandable source list
   - [ ] Add document preview links

4. **Chatbot Configuration** (2 hours)
   - [ ] Add weight configuration UI
   - [ ] Allow custom personal/school ratio
   - [ ] Show preview of knowledge sources
   - [ ] Add tooltips explaining weighting

5. **Testing** (3 hours)
   - [ ] Test hybrid search with both sources
   - [ ] Test with only personal docs
   - [ ] Test with only school docs
   - [ ] Test source attribution display
   - [ ] Performance testing

---

## Database Schema Extension

```prisma
model Chatbot {
  // ... existing fields
  personalWeight  Float?  @default(0.6)
  schoolWeight    Float?  @default(0.4)
}

model ChatMessage {
  // ... existing fields
  sources         Json?   // Array of source chunks with attribution
}
```

---

## Definition of Done

- [ ] All acceptance criteria met
- [ ] Hybrid RAG working correctly
- [ ] Source attribution displayed
- [ ] Weighting configurable
- [ ] Performance targets met (<3s)
- [ ] Both sources used when available
- [ ] Graceful handling of single source
- [ ] Code follows CLAUDE.md standards

---

**Epic 4 Complete!** 🎉
**Post-MVP Implementation Ready!**
