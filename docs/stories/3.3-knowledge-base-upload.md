# Story 3.3: Private Knowledge Base Upload

**Epic:** Epic 3 - Custom AI Chatbots
**Status:** Not Started
**Estimated Effort:** 16-20 hours
**Priority:** High
**Dependencies:** Story 3.2

---

## User Story

**As a user**, I want to upload documents to my chatbot's knowledge base, so that it can answer questions grounded in my specific materials.

---

## Story Goal

Enable users to upload documents (PDF, DOCX, TXT, MD) which will be processed, chunked, embedded using Scaleway's embedding API, and stored in PostgreSQL with pgvector for efficient similarity search.

---

## Acceptance Criteria

### File Upload
- [ ] User can upload files via drag-and-drop or file picker
- [ ] Supported formats: PDF, DOCX, TXT, MD
- [ ] Maximum file size: 10MB per file
- [ ] Multiple files can be uploaded simultaneously
- [ ] Upload progress indicator displayed
- [ ] Success/error notifications shown

### Document Processing
- [ ] Text extraction from all supported formats
- [ ] Documents chunked with 1000 token size, 200 token overlap
- [ ] Embeddings generated via Scaleway Embedding API
- [ ] Embeddings stored in PostgreSQL with pgvector
- [ ] Processing status visible to user

### Knowledge Base Management
- [ ] User can view all documents in chatbot's knowledge base
- [ ] Documents show filename, upload date, size, status
- [ ] User can delete documents
- [ ] Deleting document removes all associated chunks and embeddings

### Database
- [ ] PostgreSQL pgvector extension enabled
- [ ] Document and DocumentChunk models in Prisma
- [ ] Proper indexes for vector similarity search

---

## Dev Notes

### Prisma Schema Extensions
```prisma
model Document {
  id              String    @id @default(cuid())
  filename        String
  fileType        String
  fileSize        Int
  status          String    @default("processing")  // processing, completed, failed
  chatbotId       String
  chatbot         Chatbot   @relation(fields: [chatbotId], references: [id], onDelete: Cascade)
  chunks          DocumentChunk[]
  uploadedAt      DateTime  @default(now())

  @@index([chatbotId])
}

model DocumentChunk {
  id              String    @id @default(cuid())
  documentId      String
  document        Document  @relation(fields: [documentId], references: [id], onDelete: Cascade)
  content         String    @db.Text
  embedding       Unsupported("vector(384)")  // pgvector type, 384 dimensions
  chunkIndex      Int
  tokenCount      Int

  @@index([documentId])
}
```

### Enable pgvector
```sql
-- Run in PostgreSQL
CREATE EXTENSION IF NOT EXISTS vector;
```

### Document Processing Pipeline

```typescript
// lib/document-processor.ts
import { PDFExtract } from 'pdf.js-extract';
import mammoth from 'mammoth';
import { encode } from 'gpt-tokenizer';

export async function extractText(file: File): Promise<string> {
  const fileType = file.type;

  if (fileType === 'application/pdf') {
    return extractPDF(file);
  } else if (fileType === 'application/vnd.openxmlformats-officedocument.wordprocessingml.document') {
    return extractDOCX(file);
  } else if (fileType === 'text/plain' || fileType === 'text/markdown') {
    return file.text();
  }

  throw new Error('Unsupported file type');
}

export function chunkText(text: string, chunkSize: number = 1000, overlap: number = 200): string[] {
  const tokens = encode(text);
  const chunks: string[] = [];

  for (let i = 0; i < tokens.length; i += chunkSize - overlap) {
    const chunkTokens = tokens.slice(i, i + chunkSize);
    // Convert back to text (pseudo-code, actual implementation depends on tokenizer)
    const chunkText = decodeTokens(chunkTokens);
    chunks.push(chunkText);
  }

  return chunks;
}
```

### Scaleway Embedding Integration

```typescript
// lib/embeddings.ts
const SCALEWAY_EMBEDDING_ENDPOINT = `https://api.scaleway.ai/v1/embeddings`;
const EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2';

export async function generateEmbedding(text: string): Promise<number[]> {
  const response = await fetch(SCALEWAY_EMBEDDING_ENDPOINT, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${process.env.SCALEWAY_API_KEY}`,
    },
    body: JSON.stringify({
      model: EMBEDDING_MODEL,
      input: text,
    }),
  });

  if (!response.ok) {
    throw new Error('Failed to generate embedding');
  }

  const data = await response.json();
  return data.data[0].embedding;  // 384-dimensional vector
}

export async function generateBatchEmbeddings(texts: string[]): Promise<number[][]> {
  // Batch processing for efficiency
  const response = await fetch(SCALEWAY_EMBEDDING_ENDPOINT, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${process.env.SCALEWAY_API_KEY}`,
    },
    body: JSON.stringify({
      model: EMBEDDING_MODEL,
      input: texts,
    }),
  });

  if (!response.ok) {
    throw new Error('Failed to generate embeddings');
  }

  const data = await response.json();
  return data.data.map((item: any) => item.embedding);
}
```

### API Endpoints

**POST /api/knowledge/upload**
```typescript
// app/api/knowledge/upload/route.ts
import { NextRequest, NextResponse } from 'next/server';
import { getServerSession } from 'next-auth';
import { prisma } from '@/lib/prisma';
import { extractText, chunkText } from '@/lib/document-processor';
import { generateBatchEmbeddings } from '@/lib/embeddings';

export async function POST(request: NextRequest) {
  try {
    const session = await getServerSession();
    if (!session?.user?.id) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }

    const formData = await request.formData();
    const file = formData.get('file') as File;
    const chatbotId = formData.get('chatbotId') as string;

    // Validate chatbot ownership
    const chatbot = await prisma.chatbot.findFirst({
      where: { id: chatbotId, userId: session.user.id },
    });

    if (!chatbot) {
      return NextResponse.json({ error: 'Chatbot not found' }, { status: 404 });
    }

    // Validate file
    if (file.size > 10 * 1024 * 1024) {
      return NextResponse.json({ error: 'File too large (max 10MB)' }, { status: 400 });
    }

    // Create document record
    const document = await prisma.document.create({
      data: {
        filename: file.name,
        fileType: file.type,
        fileSize: file.size,
        chatbotId,
        status: 'processing',
      },
    });

    // Process asynchronously (consider using job queue in production)
    processDocument(document.id, file).catch((error) => {
      console.error('Error processing document:', error);
      prisma.document.update({
        where: { id: document.id },
        data: { status: 'failed' },
      });
    });

    return NextResponse.json({ data: document }, { status: 201 });
  } catch (error) {
    console.error('Error uploading document:', error);
    return NextResponse.json({ error: 'Failed to upload document' }, { status: 500 });
  }
}

async function processDocument(documentId: string, file: File) {
  // Extract text
  const text = await extractText(file);

  // Chunk text
  const chunks = chunkText(text);

  // Generate embeddings
  const embeddings = await generateBatchEmbeddings(chunks);

  // Store chunks with embeddings
  const chunkData = chunks.map((content, index) => ({
    documentId,
    content,
    embedding: embeddings[index],
    chunkIndex: index,
    tokenCount: encode(content).length,
  }));

  await prisma.documentChunk.createMany({
    data: chunkData,
  });

  // Update document status
  await prisma.document.update({
    where: { id: documentId },
    data: { status: 'completed' },
  });
}
```

---

## Tasks

1. **Database Setup** (2 hours)
   - [ ] Enable pgvector extension in PostgreSQL
   - [ ] Add Document and DocumentChunk models to Prisma
   - [ ] Create migration
   - [ ] Create vector similarity index

2. **Document Processing Library** (4 hours)
   - [ ] Create `lib/document-processor.ts`
   - [ ] Implement PDF text extraction
   - [ ] Implement DOCX text extraction
   - [ ] Implement chunking algorithm
   - [ ] Test with various document types

3. **Embedding Integration** (3 hours)
   - [ ] Create `lib/embeddings.ts`
   - [ ] Integrate Scaleway Embedding API
   - [ ] Implement batch embedding generation
   - [ ] Add error handling and retries

4. **Upload API** (3 hours)
   - [ ] Create `app/api/knowledge/upload/route.ts`
   - [ ] Implement file upload handling
   - [ ] Add validation (size, type)
   - [ ] Implement async processing

5. **Knowledge Base API** (2 hours)
   - [ ] Create `app/api/knowledge/[chatbotId]/route.ts` (GET)
   - [ ] Create `app/api/knowledge/[documentId]/route.ts` (DELETE)

6. **Frontend Components** (4 hours)
   - [ ] Create `components/chatbot/KnowledgeBaseManager.tsx`
   - [ ] Implement drag-and-drop upload
   - [ ] Show upload progress
   - [ ] Display document list
   - [ ] Add delete functionality

7. **Testing** (2 hours)
   - [ ] Test all file types
   - [ ] Test error scenarios
   - [ ] Test embedding generation
   - [ ] Verify vector storage

---

## Required NPM Packages
```bash
npm install pdf.js-extract mammoth gpt-tokenizer
npm install --save-dev @types/pdf.js-extract
```

---

## Environment Variables
```bash
SCALEWAY_API_KEY=xxx
SCALEWAY_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
MAX_UPLOAD_SIZE_MB=10
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

---

## Definition of Done

- [ ] All acceptance criteria met
- [ ] pgvector extension enabled
- [ ] All file types supported
- [ ] Embeddings generated successfully
- [ ] Vectors searchable in PostgreSQL
- [ ] Error handling implemented
- [ ] Code follows CLAUDE.md standards

---

**Next Story:** 3.4 - Chatbot Interaction with Knowledge
